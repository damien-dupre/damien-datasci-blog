+++
date = 2020-07-22T14:50:00  # Schedule page publish date.

title = "Facial and Physiological Expressions of Emotion, with a Look at Bias"
time_start = 2020-07-22T14:50:00
time_end = 2020-07-22T15:20:00
abstract = ""
abstract_short = ""
event = "CX Emotion, Emotion Impact for Consumers & Markets"
event_url = "https://cx-emotion.com"
location = "Online"

# Is this a selected talk? (true/false)
selected = false

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter the filename (excluding '.md') of your project file in `content/project/`.
#projects = ["deep-learning"]

# Links (optional).
url_pdf = ""
url_slides = "https://damien-dupre.github.io/cx_emotion_2020/slides/slides"
url_video = ""
url_code = ""

# Does the content use math formatting?
math = true

# Does the content use source code highlighting?
highlight = true

# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
[header]
image = ""
caption = ""

+++

ABSTRACT: Emotions are a main driver of decision making processes; their understanding is essential to produce consumer’s insights. With the development of IoT and Machine Learning, it is now possible to automatically evaluate emotions from various data streams among which facial expressions are one of the most prominent. An exponential number of tech companies are providing commercial systems to infer emotions from facial expressions (Software, API or SDK, see Dupré, Andelic, Morrison, & McKeown, 2018). As shown in the largest benchmark to date (Dupré, Krumhuber, Küster, & McKeown, 2019), results from these technologies are significantly less accurate than human observers. Criticising not only the algorithms’ performance but also the theory underlying these systems, well known scientists in psychology and computer science have called for a halt to the use of these technologies for significant decisions (e.g., in human resources management). However, some encouraging improvements are suggesting solutions to the frailties in emotion recognition technologies. Instead of recognizing a specific set of emotions, some systems are recognizing valence and arousal dimensions which are more generalizable features. Moreover, some research has provided specificity about the lack of accuracy in the technologies. It seems that the prediction of emotions categories does not cope well with blended emotions. Systems have to learn from more diverse and more complex emotions. Finally, with the advancement of machine learning, recognition technologies will use not only the face, but also the entire body and the context in which the expression is produced to accurately infer emotions.
